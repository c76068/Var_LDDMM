///////////////////////////////////////
/////////// CUDA KERNEL ///////////////
///////////////////////////////////////

template < typename TYPE, int DIMPOINT, int DIMVECT, KernelFun KernelFp > // Typically, float32, D, E, GaussFp
__global__ void KernelGpuGradConvXBOnDevice(TYPE ooSigma2, // 1/sigma^2
        TYPE *e,                                   // N-by-D array
        TYPE *alpha, TYPE *x, TYPE *y, TYPE *beta, // N-by-E, N-by-D, M-by-D, M-by-E arrays
        TYPE *gamma,                               // Output variable, M-by-E (same as beta)
        int nx, int ny) {
    // Thread kernel:
    // Computation of gamma_j = \partial_{beta_j} < e_i, \partial_{x_i} < alpha_i, sum_j k(x_i,y_j)*beta_j > >
    //                        = \sum_i 2* f_s'( |x_i-y_j|^2 ) * < e_i, x_i-y_j> * a_i
    // for index j given by thread id.

    int j = blockIdx.x * blockDim.x + threadIdx.x;

    // the following line does not work with nvcc 3.0 (it is a bug; it works with anterior and posterior versions)
    // extern __shared__ TYPE SharedData[];  // shared data will contain x and alpha data for the block
    // here is the bug fix (see http://forums.nvidia.com/index.php?showtopic=166905)
    extern __shared__ char SharedData_char[];
    TYPE* const SharedData = reinterpret_cast<TYPE*>(SharedData_char);
    // end of bug fix

    // One thread = One column = One y_j + One gamma_j + a whole bunch of "e_i", "a_i", "x_i".
    TYPE yj[DIMPOINT], xmy[DIMPOINT], gammaj[DIMVECT];
    if(j<ny) { // we will compute gammaj only if j is in the range
        for(int k=0; k<DIMPOINT; k++)
            yj[k]     = y[j*DIMPOINT+k]; // load y_j from device global memory
        for(int k=0; k<DIMVECT; k++)     // output : M-by-E : DIMVECT
            gammaj[k] = 0.0f;            // Make sure to put to zero the output array
    }

    // Here, we use a tiled matrix decomposition. See cuda_conv.cu for graphs and explanations.
    // Note that here, each thread reads from top to bottom (i++), instead of left to right (j++):
    for(int istart = 0, tile = 0; istart < nx; istart += blockDim.x, tile++) {

        // Load data in Shared memory -----------------------------------------------------------
        int i = tile * blockDim.x + threadIdx.x; // Current column
        // We load ei, alphai and xi from device global memory...
        if(i<nx) { // ...only if i<nx (we may be in the last rows of the last tile...)
            // Pretty uneasy to read : we store ei, alphai and xi interleaved, for better performance
            // SharedData = "[ e0, a0, x0, e1, a1, x1, e2, a2, x2, ... ]"
            int inc = DIMPOINT + DIMVECT + DIMPOINT; // Size of a  [ei, ai, xi] block
            for(int k=0; k<DIMPOINT; k++)
                SharedData[threadIdx.x*inc+k]                   =     e[i*DIMPOINT+k];
            for(int k=0; k<DIMVECT; k++)
                SharedData[threadIdx.x*inc+DIMPOINT+k]          = alpha[i*DIMVECT +k];
            for(int k=0; k<DIMPOINT; k++)
                SharedData[threadIdx.x*inc+DIMPOINT+DIMVECT+k]  =     x[i*DIMPOINT+k];
        }
        __syncthreads();
        // At this point :
        // - y_j sit in the thread memory
        // - [e_I, ..., y_{I+blockDim.x}], [a_I, ..., a_{I+blockDim.x}] and [x_I, ..., x_{I+blockDim.x}] sit
        //   in the SharedData, where [I : I+blockDim.x] is the tile span.
        // - the output line gamma_j is in the thread memory, and contains the result
        //   of the summation over the previous tiles.


        // Map-Reduction loop -------------------------------------------------------------------
        // We can now proceed to the "tiled" matrix product, where one line = one thread.
        if(j<ny) { // we compute gammaj only if j is in the range
            TYPE *ei, *alphai, *xi;            // As ei and alphai and xi are interleaved...
            ei      = SharedData;              // We'll on some cute pointer arithmetics!
            alphai  = SharedData + DIMPOINT;
            xi      = SharedData + DIMPOINT + DIMVECT;
            int inc = DIMPOINT   + DIMVECT + DIMPOINT; // The increment, size of a [ei, ai, xi] block.

            for(int irel = 0; irel < blockDim.x && irel<nx-istart; irel++, ei+=inc, alphai+=inc, xi+=inc) {
                // Reduction loop over i : we're getting to the maths ***************************
                // Remember: we're computing
                //        g_j  =  \sum_i 2* f_s'( |x_i-y_j|^2 ) * < e_i, x_i-y_j> * a_i

                TYPE r2 = 0.0f, ei_s_xmy = 0.0f;
                // Compute x_i-y_j and its squared norm:
                for(int k=0; k<DIMPOINT; k++) {
                    xmy[k]  =  xi[k]-yj[k];
                    r2     += xmy[k]*xmy[k];
                }
                // Compute < e_i, x_i-y_j> :
                for(int k=0; k<DIMPOINT; k++) // Scalar product between POINTS.
                    ei_s_xmy += ei[k]*xmy[k];
                // Scalar factor,   "2* f_s'( |x_i-y_j|^2 ) * < e_i, x_i-y_j>"
                TYPE s =  2.0f * ei_s_xmy * KernelFp( r2 , ooSigma2 );
                for(int k=0; k<DIMVECT; k++)     // Output: M-by-E
                    gammaj[k] += s * alphai[k];  // Final increment
                // ******************************************************************************
            }
        }
        // Once the loop is over, the current tiled matrix product has been reduced to gamma_i
        __syncthreads();  // So make sure that no one's left behind...
        // And move on to the next tile.
    }

    // Save the result in global memory.
    if(j<ny)
        for(int k=0; k<DIMVECT; k++)        // Remember: the output, here, is M-by-E (-> DIMVECT)
            gamma[j*DIMVECT+k] = gammaj[k];
}


